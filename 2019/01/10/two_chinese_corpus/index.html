<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>两个大规模中文语料库介绍以及处理 - jdhao's digital space</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.123.8 with theme even"><link rel=canonical href=https://jdhao.github.io/2019/01/10/two_chinese_corpus/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="两个大规模中文语料库介绍以及处理"><meta property="og:description" content="目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。"><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2019/01/10/two_chinese_corpus/"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-01-10T00:27:00+08:00"><meta property="article:modified_time" content="2020-03-11T16:29:17+01:00"><meta itemprop=name content="两个大规模中文语料库介绍以及处理"><meta itemprop=description content="目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。"><meta itemprop=datePublished content="2019-01-10T00:27:00+08:00"><meta itemprop=dateModified content="2020-03-11T16:29:17+01:00"><meta itemprop=wordCount content="1402"><meta itemprop=keywords content="Python,"><meta name=twitter:card content="summary"><meta name=twitter:title content="两个大规模中文语料库介绍以及处理"><meta name=twitter:description content="目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>两个大规模中文语料库介绍以及处理</h1><div class=post-meta><span class=post-time>2019-01-10</span><div class=post-category><a href=/categories/academic/>academic</a></div><span class=more-meta>1402 words </span><span class=more-meta>3 mins read </span><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#thucnews-语料库>THUCNews 语料库</a></li><li><a href=#中文维基百科语料库>中文维基百科语料库</a><ul><li><a href=#语料库处理>语料库处理</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><div class=post-content><p>目前进行的工作需要大规模的语料库来生成中文文本图像，因此查找资料，找了一些中文语料库。本文介绍其中的两个最大的语料库，THUCNews 语料库和中文维基百科语料库，以及如何对原始语料库文件进行简单预处理。</p><h1 id=thucnews-语料库>THUCNews 语料库</h1><p>THUCNews 语料库由清华大学自然语言处理与社会人文计算实验室收集整理发布，语料库是根据新浪新闻 RSS 订阅频道 2005~2011 年间的历史数据筛选过滤生成，包含了 836062篇文本格式的文档<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，文档编码使用 UTF-8 编码。这些文档被划分为 14 大类，分别为：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p><p>该语料库免费提供给研究者使用，下载地址为<a href=http://thuctc.thunlp.org/message>THUCNews.zip</a>。</p><p>该语料库文档都是 txt 文件，不需要再进行文本提取的处理。</p><h1 id=中文维基百科语料库>中文维基百科语料库</h1><p>维基百科会定期把各种语言的百科网页全部打包存储起来，这里我们选择其中的中文维基百科网页，这个文件可以作为中文语料库来使用。原始维基百科数据是压缩的 xml 文件，为了提取其中词条的纯文本内容，去掉众多 xml 标记，我们必须要对原始的压缩文件进行处理，提取有用信息。</p><h2 id=语料库处理>语料库处理</h2><p>首先下载原始的中文维基百科网页文件，下载地址为<a href=https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2>https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2</a>，文件大小在 1.5G 以上，在 Windows 系统下，推荐使用 <a href=https://www.freedownloadmanager.org/>Free Download Manager</a> 下载。</p><p>为了读取其中的文本信息，我们需要借助提取工具，<a href=https://github.com/attardi/wikiextractor>WikiExtractor</a> 是一款不错的开源提取工具，使用该工具，可以方便地处理语料库，输出为想要的存储格式。首先使用以下命令安装该工具：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/attardi/wikiextractor
</span></span><span class=line><span class=cl><span class=nb>cd</span> wikiextractor
</span></span><span class=line><span class=cl>python setup.py install
</span></span></code></pre></div><p>wikiextractor 会把整个语料库分割为指定大小的文件，文件的格式默认为 xml 格式，具体格式如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;doc id=&#34;xxx&#34; url=&#34;xxx&#34; title=&#34;xxxx&#34;&gt;
</span></span><span class=line><span class=cl>xxxxx
</span></span><span class=line><span class=cl>&lt;/doc&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&lt;doc id=&#34;xxx&#34; url=&#34;xxx&#34; title=&#34;xxxx&#34;&gt;
</span></span><span class=line><span class=cl>xxxxx
</span></span><span class=line><span class=cl>&lt;/doc&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&lt;doc id=&#34;xxx&#34; url=&#34;xxx&#34; title=&#34;xxxx&#34;&gt;
</span></span><span class=line><span class=cl>xxxxx
</span></span><span class=line><span class=cl>&lt;/doc&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>每个生成的文件包含若干个词条。要生成 xml 格式的语料，可以使用以下命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>WikiExtractor -o extracted_xml --process 2 -b 512K zhwiki-latest-pages-articles.xml.bz2
</span></span></code></pre></div><p><code>-o</code> 用来指定输出目录，<code>--process</code> 用来指定使用的进程数目（默认为 1），<code>-b</code> 选项用来控制单个生成文件的大小（默认为 1M，文件越大，包含的词条也越多），最后的参数为要处理的原始压缩语料文件名称。程序运行完成以后，在输出目录下面会生成多个子目录，每个目录下面有一些生成的文件。</p><p>如果要生成 json 格式的语料文件，只需要加上 <code>--json</code> 选项：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>WikiExtractor -o extracted_xml --process 2 -b 512K --json zhwiki-latest-pages-articles.xml.bz2
</span></span></code></pre></div><p>每个生成的文件中，每行对应一个以 JSON 格式存储的词条，格式如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>{&#34;id&#34;: &#34;xxx&#34;, &#34;url&#34;: &#34;xxx&#34;, &#34;title&#34;: &#34;xxx&#34;, &#34;text&#34;: &#34;xxxxxx&#34;}
</span></span></code></pre></div><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/20200119214325.png></p><p>其中 <code>text</code> 对应的是某个词条的真正内容。</p><p>如何提取 JSON 格式字符串中的 text 内容？可以使用<a href=https://docs.python.org/3/library/json.html#json.loads>json.loads()</a> 方法将符合 JSON 格式的字符串转换为 Python 中的字典。例如</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>6</span><span class=p>]:</span> <span class=n>s</span> <span class=o>=</span> <span class=s1>&#39;{&#34;apple&#34;: 1, &#34;bananas&#34;: 2, &#34;pear&#34;: 2.5}&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>7</span><span class=p>]:</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>s</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Out</span><span class=p>[</span><span class=mi>7</span><span class=p>]:</span> <span class=p>{</span><span class=s1>&#39;apple&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;bananas&#39;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;pear&#39;</span><span class=p>:</span> <span class=mf>2.5</span><span class=p>}</span>
</span></span></code></pre></div><p>下面给出一个简单读取一个文件所有词条 text 并且整合的代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>texts</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># file is a generated file</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>file</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># each line in the file is a wiki page</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>line</span> <span class=ow>in</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># read the line as valid json and select text field</span>
</span></span><span class=line><span class=cl>        <span class=n>text</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>line</span><span class=p>)[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>texts</span> <span class=o>+=</span> <span class=n>text</span>
</span></span></code></pre></div><p>根据自己的需要，可以把每一行的 text 存为新的文件，或者把原来文件里面的所有词条的 text 合在一起存为一个新文件。</p><p>另外，中文维基百科的文字很多都是繁体，如果需要简体中文，可以使用<a href=https://github.com/yichen0831/opencc-python>OpenCC</a> Python 库把繁体文字统一转换为简体再保存，具体使用方法见 Opencc 的文档，这里不再赘述。</p><h1 id=参考>参考</h1><ul><li><a href=https://www.zhihu.com/question/22956189>https://www.zhihu.com/question/22956189</a></li><li><a href=https://www.zhihu.com/question/21177095>https://www.zhihu.com/question/21177095</a></li><li><a href=https://blog.csdn.net/wangyangzhizhou/article/details/78348949>https://blog.csdn.net/wangyangzhizhou/article/details/78348949</a></li><li><a href=https://github.com/Embedding/Chinese-Word-Vectors>https://github.com/Embedding/Chinese-Word-Vectors</a></li><li>THUCNews 语料库主页：http://thuctc.thunlp.org/</li><li>中文 wiki dump 主页：https://dumps.wikimedia.org/zhwiki/</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>使用 <code>find THUCNews_ROOT -type f -name '*.txt' -print|wc -l</code> 可以查看语料库中文本文件的数目。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2020-03-11</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/Python/>Python</a></div><nav class=post-nav><a class=prev href=/2019/01/11/line_number_setting_nvim/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Line Number Settings for More Efficient Movement in Neovim</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/2019/01/07/windows_tools_for_programmers/><span class="next-text nav-default">Windows 系统下几款程序员不可不用的神器</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span><span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>