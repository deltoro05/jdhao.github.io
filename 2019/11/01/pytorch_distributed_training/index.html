<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Distributed Training in PyTorch with Horovod - jdhao's digital space</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="Horovod is the distributed training framework developed by Uber. It support training distributed programs with little modification for both TensorFlow, PyTorch, MXNet and keras.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.123.8 with theme even"><link rel=canonical href=https://jdhao.github.io/2019/11/01/pytorch_distributed_training/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Distributed Training in PyTorch with Horovod"><meta property="og:description" content="Horovod is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2019/11/01/pytorch_distributed_training/"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-11-01T22:26:53+08:00"><meta property="article:modified_time" content="2022-01-09T14:10:40+01:00"><meta itemprop=name content="Distributed Training in PyTorch with Horovod"><meta itemprop=description content="Horovod is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras."><meta itemprop=datePublished content="2019-11-01T22:26:53+08:00"><meta itemprop=dateModified content="2022-01-09T14:10:40+01:00"><meta itemprop=wordCount content="821"><meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Distributed Training in PyTorch with Horovod"><meta name=twitter:description content="Horovod is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Distributed Training in PyTorch with Horovod</h1><div class=post-meta><span class=post-time>2019-11-01</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>821 words </span><span class=more-meta>4 mins read </span><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#basic-concepts-of-mpi>Basic concepts of MPI</a><ul><li><a href=#the-meaning-of-some-collective-operations>The meaning of some collective operations</a></li><li><a href=#references>References</a></li></ul></li><li><a href=#what-are-world-size-rank-local-rank>What are world size, rank, local rank?</a></li><li><a href=#how-does-batch-size-and-multi-gpu-training-work-together>How does batch size and multi-GPU training work together?</a><ul><li><a href=#references-1>References</a></li></ul></li></ul></nav></div></div><div class=post-content><p><a href=https://github.com/horovod/horovod>Horovod</a> is the distributed training
framework developed by Uber. It support training distributed programs with
little modification for both TensorFlow, PyTorch, MXNet and keras.</p><h1 id=basic-concepts-of-mpi>Basic concepts of MPI</h1><p>For distributed training, horovod relies on MPI or Gloo, both of which are
libraries developed for parallel computing. The underlying concepts may be
similar.</p><p>Communicator: this is a common world for a group of processes. Each process
inside the communicator has its rank, which is a unique id used by MPI to
identify a particular process.</p><p>Point-to-point communication: it means that a process interacts with another
process directly in the communicator.</p><p>Collective operation: A MPI collective operation involves all the processes in
a communicator. These operations include broadcast, gather, scatter, reduce,
etc.</p><h2 id=the-meaning-of-some-collective-operations>The meaning of some collective operations</h2><ul><li>Broadcast means to copy the same data from the root process to the other processes.</li><li>Gather means to take data from other processes and put them in the root process.</li><li>Scatter means to slice the dataset in root process to several parts and distribute one part to each process.</li><li>Reduce means to perform some sort of action (sum, max, min, etc.) on the data from all processes and put the result in the root process.</li><li>Allreduce is based on reduce operation and it copies the result to other processes.</li></ul><h2 id=references>References</h2><ul><li><a href=https://www.uio.no/studier/emner/matnat/ifi/INF3380/v11/undervisningsmateriale/inf3380-week06.pdf>Introduction to MPI programming.</a></li><li><a href=https://indico.desy.de/indico/event/12535/session/2/contribution/30/material/13/0.pdf>Parallel computing with MPI.</a></li><li><a href=https://en.wikipedia.org/wiki/Message_Passing_Interface#Concepts>Wiki page on MPI concepts.</a></li></ul><h1 id=what-are-world-size-rank-local-rank>What are world size, rank, local rank?</h1><p>First, the meaning of world in distributed scenario:</p><blockquote><p>By default, collectives are executed on all the processes, also known as world.</p></blockquote><p>In PyTorch, distributed training using <code>torch.dist.DistributedParallel</code>, the
number of spawned processed equals to the number of GPUs you want to use. Rank
is the unique id given to each process, and local rank is the local id for GPUs
in the same node.</p><p>For example, if you want to use 2 nodes and 4 GPUs per node, then 2*4 =8
processes will be spawned. World size is 8. In node 1, the process rank is 0,
1, 2, 3, and in node 2, the process rank is 4, 5, 6, 7. In both the two nodes,
local rank would be 0, 1, 2, 3.</p><p>In horovod, the concept is similar. Usually, each GPU corresponds to one
process. Size is equal to the number of processes.</p><h1 id=how-does-batch-size-and-multi-gpu-training-work-together>How does batch size and multi-GPU training work together?</h1><p>In PyTorch, for single node, multi-GPU training (i.e., using
<a href=https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel><code>torch.nn.DataParallel</code></a>),
the data batch is split in the first dimension, which means that you should
multiply your original batch size (for single node single GPU training) by the
number of GPUs you want to use if you want to the original batch size for one
GPU.</p><p>For multi-node, multi-GPU training using horovod, the situation is different.
In this case, we first need to use a
<a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler><code>DistributedSampler()</code></a>
like the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>train_sampler = torch.utils.data.distributed.DistributedSampler(
</span></span><span class=line><span class=cl>    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())
</span></span></code></pre></div><p>In the above statement, the parameter <code>num_replicas</code> is the world size, and
parameter <code>rank</code> is the global rank (in contrast to the local rank) of current
process. But how does a DistributedSampler work?</p><p>You can find the source code of <code>DistributedSampler</code>
<a href=https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler>here</a>.
According to the code, roughly speaking, it splits the dataset into
<code>num_replicas</code> parts, and according to its rank, each process get a part of the
original indexes of the dataset:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>indices  =  indices[self.rank:self.total_size:self.num_replicas]
</span></span></code></pre></div><p>In the end, this sampler gets <code>len(dataset)/num_replicas</code> samples.</p><p>In the dataloader part, we need to use this distributed sampler instead of the
plain simple sampler:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>kwargs</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;num_workers&#39;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;pin_memory&#39;</span><span class=p>:</span> <span class=n>True</span><span class=p>}</span> <span class=k>if</span> <span class=n>args</span><span class=o>.</span><span class=n>cuda</span> <span class=k>else</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>sampler</span><span class=o>=</span><span class=n>train_sampler</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span></code></pre></div><p>The size of this <code>train_loader</code>, i.e., the number of iterations for a single
process, is <code>len(dataset)/(num_replicas*batch_size)</code>. In a one node one GPU
scenario, the dataloader size is usually <code>len(dataset)/batch_size</code>. So in a
distributed scenario, the number of iterations for a single process is the
number of the iterations in one node one GPU case divided by <code>num_replica</code>. By
iterating over their respective data part, all the processes have finished one
epoch in the whole dataset.</p><p>Let&rsquo;s take a concrete example to illustrate the idea. Suppose the dataset size
is 1024 and batch size is 32. In one node one GPU case, the number of
iterations in one epoch is 1024/32=32.</p><p>If we instead use two nodes with 4 GPUs for each node. In total, 2*4=8
processes are started for distributed training. In this case, each process get
1024/8=128 samples in the dataset. The number of iterations for each process in
an epoch is 128/32=4.</p><p>Now you can see why distributed training is faster because each GPU only needs
to do a fraction of the jobs when only one GPU is used. Theoretically, the
acceleration you will get is N, where N is the number of GPUs used in the
distributed training. In reality, you won&rsquo;t get that much acceleration for
your program due to communication costs and other factors.</p><h2 id=references-1>References</h2><ul><li><a href=https://discuss.pytorch.org/t/a-question-concerning-batchsize-and-multiple-gpus-in-pytorch/33767/2>https://discuss.pytorch.org/t/a-question-concerning-batchsize-and-multiple-gpus-in-pytorch/33767/2</a></li><li><a href=https://stackoverflow.com/questions/54216920/how-to-use-multiple-gpus-in-pytorch>https://stackoverflow.com/questions/54216920/how-to-use-multiple-gpus-in-pytorch</a></li><li><a href=https://github.com/horovod/horovod/blob/master/examples/pytorch_imagenet_resnet50.py>Horovod distributed training for imagenet classification.</a></li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-01-09</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/PyTorch/>PyTorch</a></div><nav class=post-nav><a class=prev href=/2019/11/02/thought_on_making_of_atomic_bomb/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">人类第一颗原子弹爆炸始末</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/2019/10/29/expect_script_learning/><span class="next-text nav-default">Learning Expect Programming</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span><span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>