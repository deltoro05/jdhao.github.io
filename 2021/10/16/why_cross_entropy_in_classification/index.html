<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Why Use Cross Entropy in Classification Task? - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="In classification tasks, the de facto loss to use is the cross entropy loss.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Why Use Cross Entropy in Classification Task?"><meta property="og:description" content="In classification tasks, the de facto loss to use is the cross entropy loss."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-10-16T09:54:49+08:00"><meta property="article:modified_time" content="2022-02-14T06:03:18+01:00"><meta itemprop=name content="Why Use Cross Entropy in Classification Task?"><meta itemprop=description content="In classification tasks, the de facto loss to use is the cross entropy loss."><meta itemprop=datePublished content="2021-10-16T09:54:49+08:00"><meta itemprop=dateModified content="2022-02-14T06:03:18+01:00"><meta itemprop=wordCount content="364"><meta itemprop=keywords content="loss,softmax,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Why Use Cross Entropy in Classification Task?"><meta name=twitter:description content="In classification tasks, the de facto loss to use is the cross entropy loss."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Why Use Cross Entropy in Classification Task?</h1><div class=post-meta><span class=post-time>2021-10-16</span><div class=post-category><a href=/categories/academic/>academic</a></div><span class=more-meta>364 words</span>
<span class=more-meta>2 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#references>References</a></li></ul></nav></div></div><div class=post-content><p>In classification tasks, the de facto loss to use is the <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>cross entropy loss</a>.</p><p>Suppose that we have 10 classes, we would like the network to predict the
probabilities of current sample belonging to each of the 10 classes. However,
the raw output from from a neural network is just floating point values. So the
<a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a> is used to normalize the output to fall in the range $(0, 1)$.</p><p>After softmax, all output values are between 0 and 1 and their sum is 1. So the
output now can be considered as the probability distribution over the predicted
classes. The element with largest probability is the predicted class.</p><p>Now, suppose we have a batch of N data samples and their class labels, from the
point of maximum likelihood estimation (or <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>MLE</a> in short), we want to find
the parameters of the neural network that can maximize the product of
probabilities that each sample get in their ground truth class.</p><p>For example, if we have data sample x1, x2 and x3, and their class label is 1,
3, 5. Then we want to find network parameters that can maximize <code>p{11} * p{23} * p{35}</code>.</p><p>Then according to theory of MLE, we need to do derivative stuff and find the
parameter. However, the multiplication form is not suitable for calculating
derivatives. That is why <code>log()</code> function used in this.</p><p>Why log function? Because:</p><ol><li>log function is monotonic. Or our objective is the same. Maximizing the old
objective is equivalent to maximizing the new objective.</li><li>log(x * y) = log(x) + log(y), so that we can greatly simplify calculation of derivatives.</li></ol><p>Another question, why do we use the minus sign?</p><p>Because in machine learning, we always talk about minimizing the loss/cost,
which is equivalent to maximize log likelihood. It is just a convention. By
adding a minus sign, we transform the initial problem of maximizing likelihood
to minimize the new loss function. They are essentially the same, but loss
function is a more familiar jargon to machine learning practitioners.</p><p>The cross entropy loss is also called log loss.</p><p>To be continued&mldr;</p><h1 id=references>References</h1><ul><li><a href=https://stats.stackexchange.com/questions/436766/cross-entropy-with-log-softmax-activation>https://stats.stackexchange.com/questions/436766/cross-entropy-with-log-softmax-activation</a></li><li><a href=https://stats.stackexchange.com/questions/366312/why-we-use-log-function-for-cross-entropy>https://stats.stackexchange.com/questions/366312/why-we-use-log-function-for-cross-entropy</a></li><li><a href=https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a>https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a</a></li><li><a href=https://arxiv.org/abs/1702.05659>On Loss Functions for Deep Neural Networks in Classification</a></li><li><a href=https://paperswithcode.com/paper/demystifying-loss-functions-for>https://paperswithcode.com/paper/demystifying-loss-functions-for</a></li><li><a href=https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/>https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/</a></li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-02-14</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/loss/>loss</a>
<a href=/tags/softmax/>softmax</a></div><nav class=post-nav><a class=prev href=/2021/10/20/python_apsscheduler_intro/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Scheduling Your Tasks with Package Apscheduler</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2021/10/06/yasnippet_setup_emacs/><span class="next-text nav-default">Setting up Yasnippet for Emacs</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>