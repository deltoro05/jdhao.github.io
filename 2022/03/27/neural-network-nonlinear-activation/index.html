<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Nonlinear Activations for Neural Networks - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content=" Non-linear activations are important in deep neural networks. It is important in the sense that without non-linear activation functions, even if you have many linear layers, the end results is like you have only one linear layer, and the approximation ability of the network is very limited1. Some of most commonly-used nonlinear activation functions are Sigmoid, ReLU and Tanh.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Nonlinear Activations for Neural Networks"><meta property="og:description" content="


Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited1. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2022/03/27/neural-network-nonlinear-activation/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-03-27T17:25:12+08:00"><meta property="article:modified_time" content="2022-03-27T17:50:40+02:00"><meta itemprop=name content="Nonlinear Activations for Neural Networks"><meta itemprop=description content="


Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited1. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh."><meta itemprop=datePublished content="2022-03-27T17:25:12+08:00"><meta itemprop=dateModified content="2022-03-27T17:50:40+02:00"><meta itemprop=wordCount content="586"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Nonlinear Activations for Neural Networks"><meta name=twitter:description content="


Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited1. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Nonlinear Activations for Neural Networks</h1><div class=post-meta><span class=post-time>2022-03-27</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>586 words</span>
<span class=more-meta>3 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content></div></div><div class=post-content><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/202203271740705.png width=600></p><p>Non-linear activations are important in deep neural networks. It is
important in the sense that without non-linear activation functions,
even if you have many linear layers, the end results is like you have
only one linear layer, and the approximation ability of the network is
very limited<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a>. Some of most commonly-used
nonlinear activation functions are Sigmoid, ReLU and Tanh.</p><h1 id=nonlinear-activations-and-their-derivatives>Nonlinear
activations and their derivatives</h1><h2 id=sigmoid>Sigmoid</h2><p>Sigmoid function, also known as logistic function, has the following
form:</p><p><span class="math display">\[f(x) = \frac{1}{1+e^{-x}}\]</span></p><p>The derivative of sigmoid is:</p><p><span class="math display">\[\begin{aligned}\frac{df}{dx} &=
\frac{e^{-x}}{(1+e^{-x})^2}\\
&= \frac{1}{1+e^{-x}}(1-
\frac{1}{1+e^{-x}})\\
&= f(x)(1-f(x))
\end{aligned}\]</span></p><h2 id=tanh>Tanh</h2><p><a href=https://en.wikipedia.org/wiki/Hyperbolic_functions#Exponential_definitions>Tanh</a>
function</p><p><span class="math display">\[f(x) =
\frac{e^{2x}-1}{e^{2x}+1}\]</span></p><p>The derivative of Tanh is:</p><p><span class="math display">\[\frac{df}{dx} = \frac{4e^{2x}}{(e^{2x} +
1)^2} = 1 - {f(x)}^2\]</span></p><h2 id=relu>ReLU</h2><p>ReLU, called rectified linear unit, has the following form:</p><p><span class="math display">\[f(x) = \max(0, x)\]</span></p><p>We can also write ReLU as:</p><p><span class="math display">\[f(x) =
\begin{cases}
x & x \geq 0 \\
0 & x &lt; 0
\end{cases}\]</span></p><p>The derivate of ReLU is quite simple, it is <code>1</code> for <span class="math inline">\(x > 0\)</span> and 0 otherwise.</p><p>There are also variants of ReLU, such as Leaky ReLU, PReLU
(parametric ReLU), and RReLU (randomized ReLU). In <a href=https://arxiv.org/abs/1505.00853>Empirical Evaluation of
Rectified Activations in Convolutional Network</a>, the author claimed
that PReLU and RReLU works better than ReLU in small scale datasets such
as CIFAR10, CIFAR100 and <a href=https://www.kaggle.com/c/datasciencebowl>Kaggle NDSB</a>.</p><h1 id=vanishing-gradient>Vanishing gradient</h1><p>I show the plot of different activation functions and their
derivatives in the title image.</p><details><summary><font size=2 color=red>Click to show the code for
visualization.</font></summary><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1 aria-hidden=true tabindex=-1></a><span class=im>import</span> matplotlib.pyplot <span class=im>as</span> plt</span>
<span id=cb1-2><a href=#cb1-2 aria-hidden=true tabindex=-1></a><span class=im>import</span> numpy <span class=im>as</span> np</span>
<span id=cb1-3><a href=#cb1-3 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-4><a href=#cb1-4 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-5><a href=#cb1-5 aria-hidden=true tabindex=-1></a><span class=kw>def</span> main():</span>
<span id=cb1-6><a href=#cb1-6 aria-hidden=true tabindex=-1></a>    x <span class=op>=</span> np.linspace(<span class=op>-</span><span class=dv>5</span>, <span class=dv>5</span>, <span class=dv>100</span>)</span>
<span id=cb1-7><a href=#cb1-7 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-8><a href=#cb1-8 aria-hidden=true tabindex=-1></a>    r <span class=op>=</span> [relu(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-9><a href=#cb1-9 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-10><a href=#cb1-10 aria-hidden=true tabindex=-1></a>    sig <span class=op>=</span> [sigmoid(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-11><a href=#cb1-11 aria-hidden=true tabindex=-1></a>    d_sig <span class=op>=</span> [sigmoid(v)<span class=op>*</span>(<span class=dv>1</span> <span class=op>-</span> sigmoid(v)) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-12><a href=#cb1-12 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-13><a href=#cb1-13 aria-hidden=true tabindex=-1></a>    t <span class=op>=</span> [tanh(v) <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-14><a href=#cb1-14 aria-hidden=true tabindex=-1></a>    d_tanh <span class=op>=</span> [<span class=dv>1</span> <span class=op>-</span> tanh(v)<span class=op>**</span><span class=dv>2</span> <span class=cf>for</span> v <span class=kw>in</span> x]</span>
<span id=cb1-15><a href=#cb1-15 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-16><a href=#cb1-16 aria-hidden=true tabindex=-1></a>    fig <span class=op>=</span> plt.figure(figsize<span class=op>=</span>[<span class=dv>6</span>, <span class=dv>3</span>])</span>
<span id=cb1-17><a href=#cb1-17 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-18><a href=#cb1-18 aria-hidden=true tabindex=-1></a>    ax <span class=op>=</span> fig.add_subplot(<span class=dv>111</span>)</span>
<span id=cb1-19><a href=#cb1-19 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-20><a href=#cb1-20 aria-hidden=true tabindex=-1></a>    ax.plot(x, r, <span class=st>&#39;#66c2a5&#39;</span>, label<span class=op>=</span><span class=st>&#39;ReLU&#39;</span>)</span>
<span id=cb1-21><a href=#cb1-21 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-22><a href=#cb1-22 aria-hidden=true tabindex=-1></a>    ax.plot(x, sig, <span class=st>&#39;#fc8d62&#39;</span>, label<span class=op>=</span><span class=st>&#39;sigmoid&#39;</span>)</span>
<span id=cb1-23><a href=#cb1-23 aria-hidden=true tabindex=-1></a>    ax.plot(x, d_sig, <span class=st>&#39;#8da0cb&#39;</span>, label<span class=op>=</span><span class=st>&#39;sigmoid derivative&#39;</span>)</span>
<span id=cb1-24><a href=#cb1-24 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-25><a href=#cb1-25 aria-hidden=true tabindex=-1></a>    ax.plot(x, t, <span class=st>&#39;#e78ac3&#39;</span>, label<span class=op>=</span><span class=st>&#39;tanh&#39;</span>)</span>
<span id=cb1-26><a href=#cb1-26 aria-hidden=true tabindex=-1></a>    ax.plot(x, d_tanh, <span class=st>&#39;#a6d854&#39;</span>, label<span class=op>=</span><span class=st>&#39;tanh derivative&#39;</span>)</span>
<span id=cb1-27><a href=#cb1-27 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-28><a href=#cb1-28 aria-hidden=true tabindex=-1></a>    ax.legend()</span>
<span id=cb1-29><a href=#cb1-29 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-30><a href=#cb1-30 aria-hidden=true tabindex=-1></a>    plt.savefig(<span class=st>&#39;activation-curve.png&#39;</span>, dpi<span class=op>=</span><span class=dv>96</span>, bbox_inches<span class=op>=</span><span class=st>&#39;tight&#39;</span>)</span>
<span id=cb1-31><a href=#cb1-31 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-32><a href=#cb1-32 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-33><a href=#cb1-33 aria-hidden=true tabindex=-1></a><span class=kw>def</span> relu(x):</span>
<span id=cb1-34><a href=#cb1-34 aria-hidden=true tabindex=-1></a>    <span class=cf>if</span> x <span class=op>&gt;=</span><span class=dv>0</span>:</span>
<span id=cb1-35><a href=#cb1-35 aria-hidden=true tabindex=-1></a>        <span class=cf>return</span> x</span>
<span id=cb1-36><a href=#cb1-36 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-37><a href=#cb1-37 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> <span class=dv>0</span></span>
<span id=cb1-38><a href=#cb1-38 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-39><a href=#cb1-39 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-40><a href=#cb1-40 aria-hidden=true tabindex=-1></a><span class=kw>def</span> sigmoid(x):</span>
<span id=cb1-41><a href=#cb1-41 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> <span class=dv>1</span><span class=op>/</span>(<span class=dv>1</span> <span class=op>+</span> np.exp(<span class=op>-</span>x))</span>
<span id=cb1-42><a href=#cb1-42 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-43><a href=#cb1-43 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-44><a href=#cb1-44 aria-hidden=true tabindex=-1></a><span class=kw>def</span> tanh(x):</span>
<span id=cb1-45><a href=#cb1-45 aria-hidden=true tabindex=-1></a>    <span class=cf>return</span> (np.exp(x)<span class=op>**</span><span class=dv>2</span> <span class=op>-</span> <span class=dv>1</span>)<span class=op>/</span>(np.exp(x)<span class=op>**</span><span class=dv>2</span> <span class=op>+</span> <span class=dv>1</span>)</span>
<span id=cb1-46><a href=#cb1-46 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-47><a href=#cb1-47 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-48><a href=#cb1-48 aria-hidden=true tabindex=-1></a><span class=cf>if</span> <span class=va>__name__</span> <span class=op>==</span> <span class=st>&quot;__main__&quot;</span>:</span>
<span id=cb1-49><a href=#cb1-49 aria-hidden=true tabindex=-1></a>    main()</span></code></pre></div></details><p>The derivative of sigmoid is relatively small, and its largest value
is only 0.25 (when <span class="math inline">\(x = 0\)</span>). When
<span class="math inline">\(x\)</span> is large, the derivative is near
zero. Tanh has a similar issue: it has a low gradient, and maximum
gradient is only 1 (<span class="math inline">\(x=0\)</span>).</p><p>This will cause the vanishing gradient problem, because in order to
calculate the derivative of loss w.r.t the weight of earlier layers in
the network, we need to multiply the gradient in the later layers. When
you multiply several values below 0.25, the result goes down to zero
quickly, so the network weight in earlier layers get updated slowly. In
other words, the learning process will converge much slower than using
ReLU, and we might need much more epochs to get a satisfactory
result.</p><p>Another advantage of ReLU is that it is computationally cheap
compared to sigmoid, both in terms of forward and backward
operation.</p><h1 id=try-it-yourself-interactively>Try it yourself
interactively</h1><p>To gain more insight into this, we can use <a href=https://cs.stanford.edu/~karpathy/convnetjs/demo/mnist.html>minist
on convenet.js</a> and change the activation function to see how the
train goes. We can see that training process under tanh and sigmoid
activation is much slower than ReLU. Sigmoid is slowest among the
three.</p><p>We can also play with different activations functions real quick with
<a href=https://playground.tensorflow.org/>TensorFlow
playground</a>.</p><h1 id=references>References</h1><ul><li><a href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks) class=uri>https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></li><li>A list of activation functions: <a href=https://en.wikipedia.org/wiki/Activation_function class=uri>https://en.wikipedia.org/wiki/Activation_function</a></li><li>ReLU vs Sigmoid: <a href=https://stats.stackexchange.com/q/126238/140049 class=uri>https://stats.stackexchange.com/q/126238/140049</a></li></ul><section id=footnotes class="footnotes footnotes-end-of-document" role=doc-endnotes><hr><ol><li id=fn1><p>See <a href=https://stats.stackexchange.com/a/335972/140049>this post</a> and
also <a href=https://stackoverflow.com/q/9782071/6064933>this one</a>
for more detailed discussions.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-03-27</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/2022/03/28/how_to_make_zhajiang2/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">地道美味炸酱制作方法</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2022/03/18/torch_accelerate_batch_inference/><span class="next-text nav-default">Accelerate Batched Image Inference in PyTorch</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>