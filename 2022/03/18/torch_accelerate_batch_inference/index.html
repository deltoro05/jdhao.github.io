<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Accelerate Batched Image Inference in PyTorch - jdhao's digital space</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time. Initially, I use a naive approach and just transform the images one by one, then combine them to form a single tensor and do the inference. The inference becomes really slow when I have a batch with more than 30 images. The inference time is about 1-2 seconds per batch.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.123.8 with theme even"><link rel=canonical href=https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Accelerate Batched Image Inference in PyTorch"><meta property="og:description" content="I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2022/03/18/torch_accelerate_batch_inference/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-03-18T22:59:33+08:00"><meta property="article:modified_time" content="2022-03-26T09:40:37+01:00"><meta itemprop=name content="Accelerate Batched Image Inference in PyTorch"><meta itemprop=description content="I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch."><meta itemprop=datePublished content="2022-03-18T22:59:33+08:00"><meta itemprop=dateModified content="2022-03-26T09:40:37+01:00"><meta itemprop=wordCount content="511"><meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Accelerate Batched Image Inference in PyTorch"><meta name=twitter:description content="I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Accelerate Batched Image Inference in PyTorch</h1><div class=post-meta><span class=post-time>2022-03-18</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>511 words </span><span class=more-meta>3 mins read </span><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#should-we-set-cudnnbenchmark-to-true>Should we set cudnn.benchmark to True?</a></li><li><a href=#dataset-and-dataloader-for-inference>Dataset and DataLoader for inference</a></li><li><a href=#use-torchcudasynchronize-for-correct-benchmarking>Use torch.cuda.synchronize() for correct benchmarking</a></li><li><a href=#important-parameters>Important parameters</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=post-content><p>I have a web service where the images come in a batch so I have to do inference for several images in PIL format at a time.
Initially, I use a naive approach and just transform the images one by one,
then combine them to form a single tensor and do the inference.
The inference becomes really slow when I have a batch with more than 30 images.
The inference time is about 1-2 seconds per batch.</p><h1 id=should-we-set-cudnnbenchmark-to-true>Should we set cudnn.benchmark to True?</h1><p>Some blog posts have recommend an easy way to speed your inference: setting <code>torch.backends.cudnn.benchmark</code> to <code>True</code>.
By setting this option to <code>True</code>, cudnn will try to find the fastest convolution algorithm for your input shape.
However, this only works when the input shape to the model does not change.
If the input shape changes, the time cost will actually be worse<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><h1 id=dataset-and-dataloader-for-inference>Dataset and DataLoader for inference</h1><p>After some debugging, I found that data transformation may be the bottleneck.
In the naive approach, the data processing for the images are done sequentially,
sometime like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>processed_imgs</span> <span class=o>=</span> <span class=p>[</span><span class=n>transform</span><span class=p>(</span><span class=n>im</span><span class=p>)</span> <span class=k>for</span> <span class=n>im</span> <span class=ow>in</span> <span class=n>pil_imgs</span><span class=p>]</span>
</span></span></code></pre></div><p>Actually we can use <code>DataLoader</code> from torch to accelerate the image processing speed.
We need to define a <code>Dataset</code> and <code>DataLoader</code> for the inference.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>InferDataset</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>pil_imgs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>InferDataset</span><span class=p>,</span> <span class=bp>self</span><span class=p>,)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pil_imgs</span> <span class=o>=</span> <span class=n>pil_imgs</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transform</span> <span class=o>=</span> <span class=n>make_transform</span><span class=p>()</span> <span class=c1># some infer transform</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>pil_imgs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__getitem__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>img</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pil_imgs</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>infer_data</span> <span class=o>=</span> <span class=n>InferDataset</span><span class=p>(</span><span class=n>pil_imgs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>infer_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>infer_data</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                           <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                           <span class=n>num_workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                           <span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>infer_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># ... more processing</span>
</span></span></code></pre></div><h1 id=use-torchcudasynchronize-for-correct-benchmarking>Use torch.cuda.synchronize() for correct benchmarking</h1><p>Note that the torch cuda operations are asynchronous, which will return without waiting to finish.
To time a cuda operation correctly, we need to use <code>torch.cuda.synchronize()</code> to wait for the operation to finish.
So the timing code should be like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># your cuda operations go here, for example, out = mode(input)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;elapse: </span><span class=si>{</span><span class=n>end</span><span class=o>-</span><span class=n>start</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=important-parameters>Important parameters</h1><p>The parameters that impact the speed most are <code>batch_size</code> and <code>num_workers</code>.</p><p>If GPU memory permits, using a large batch size will be faster since we have fewer iterations to run.
The exact value for batch size should be benchmarked on your system.</p><p>The parameter <code>num_worker</code> means the number of worker processes used for fetching data.
When it is 0, only the main processes will be used, which will be slow.
However, it does not mean more workers will definitely lead to faster processing speed.
We need to benchmark and choose a suitable value.
Generally, it should not exceed the number of CPU cores we have.
For example, I found that setting <code>num_workers</code> to 1 works the fastest for me.</p><p>The parameter <code>pin_memory=True</code> will reduce the time cost for transferring data from your CPU to GPU
(detail <a href=https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/>here</a>), thus accelerating data processing.
So in generally, it should be always used.</p><h1 id=conclusion>Conclusion</h1><p>With all these optimizations, I was able to reduce the batched image inference time from 2 seconds to about 100 ms.</p><h1 id=references>References</h1><ul><li><a href=https://www.willprice.dev/2021/03/27/debugging-pytorch-performance-bottlenecks.html>https://www.willprice.dev/2021/03/27/debugging-pytorch-performance-bottlenecks.html</a></li><li><a href=https://leimao.github.io/blog/PyTorch-Benchmark/>https://leimao.github.io/blog/PyTorch-Benchmark/</a></li><li><a href=https://auro-227.medium.com/timing-your-pytorch-code-fragments-e1a556e81f2>https://auro-227.medium.com/timing-your-pytorch-code-fragments-e1a556e81f2</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>See also discussion <a href="https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/31?u=jdhao">here</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-03-26</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/PyTorch/>PyTorch</a></div><nav class=post-nav><a class=prev href=/2022/03/27/neural-network-nonlinear-activation/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Nonlinear Activations for Neural Networks</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/2022/03/14/latex_define_string_variable/><span class="next-text nav-default">Define A String Variable in LaTeX</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span><span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>