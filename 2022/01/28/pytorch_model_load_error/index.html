<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>A Dig into PyTorch Model Loading - jdhao's digital space</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="Saving and loading PyTorch models Models in PyTorch are a subclass of torch.nn.Module. To save the model parameters, we use model.state_dict() to get all the model parameters:
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.123.8 with theme even"><link rel=canonical href=https://jdhao.github.io/2022/01/28/pytorch_model_load_error/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="A Dig into PyTorch Model Loading"><meta property="og:description" content="Saving and loading PyTorch models
Models in PyTorch are a subclass of torch.nn.Module. To save the model parameters,
we use model.state_dict() to get all the model parameters:"><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2022/01/28/pytorch_model_load_error/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-01-28T23:17:45+08:00"><meta property="article:modified_time" content="2022-01-28T16:30:22+01:00"><meta itemprop=name content="A Dig into PyTorch Model Loading"><meta itemprop=description content="Saving and loading PyTorch models
Models in PyTorch are a subclass of torch.nn.Module. To save the model parameters,
we use model.state_dict() to get all the model parameters:"><meta itemprop=datePublished content="2022-01-28T23:17:45+08:00"><meta itemprop=dateModified content="2022-01-28T16:30:22+01:00"><meta itemprop=wordCount content="280"><meta itemprop=keywords content="PyTorch,"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Dig into PyTorch Model Loading"><meta name=twitter:description content="Saving and loading PyTorch models
Models in PyTorch are a subclass of torch.nn.Module. To save the model parameters,
we use model.state_dict() to get all the model parameters:"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>A Dig into PyTorch Model Loading</h1><div class=post-meta><span class=post-time>2022-01-28</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>280 words </span><span class=more-meta>2 mins read </span><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#saving-and-loading-pytorch-models>Saving and loading PyTorch models</a></li><li><a href=#loading-error-when-using-torchload-to-load-model-trained-on-gpu>Loading error when using torch.load to load model trained on GPU</a></li><li><a href=#the-cause>The cause</a></li><li><a href=#load-the-model-correctly>Load the model correctly</a></li><li><a href=#ref>ref</a></li></ul></nav></div></div><div class=post-content><h1 id=saving-and-loading-pytorch-models>Saving and loading PyTorch models</h1><p>Models in PyTorch are a subclass of <code>torch.nn.Module</code>. To save the model parameters,
we use <code>model.state_dict()</code> to get all the model parameters:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span></code></pre></div><p>Then save the model parameter using <code>torch.save()</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=s1>&#39;my-model.pth&#39;</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=loading-error-when-using-torchload-to-load-model-trained-on-gpu>Loading error when using torch.load to load model trained on GPU</h1><p>When we load a model trained on GPU in a machine with no GPU using <code>torch.load(model_path)</code>,
we often get the following error:</p><blockquote><p>RuntimeError: Attempting to deserialize object on a CUDA device but
torch.cuda.is_available() is False. If you are running on a CPU-only machine,
please use torch.load with map_location=torch.device(&lsquo;cpu&rsquo;) to map your
storages to the CPU.</p></blockquote><h1 id=the-cause>The cause</h1><p>This is because when we use <a href=https://pytorch.org/docs/stable/generated/torch.save.html#torch-save><code>torch.save()</code></a> to save an object,
torch will also store the location of original data (called location tag, check <a href=https://github.com/pytorch/pytorch/blob/be2dc8f2940d3c95941516a811be8c504910d1ea/torch/serialization.py#L574>code here</a>).
<code>torch.save()</code> also keeps the view relationship between tensors unchanged, see <a href=https://pytorch.org/docs/stable/notes/serialization.html#saving-and-loading-tensors-preserves-views>here</a>.</p><p>Based on <a href=https://github.com/pytorch/pytorch/blob/be2dc8f2940d3c95941516a811be8c504910d1ea/torch/serialization.py#L600-L601>code here</a>, it seems that PyTorch will save the GPU tensor as CPU.</p><p>When we use <a href=https://pytorch.org/docs/stable/generated/torch.load.html#torch.load><code>torch.load()</code></a>, since the tensor location has been recorded,
torch will load the tensor first to CPU, then moves it to the GPU indicated by the location tag.
If that GPU is missing or we are using a CPU machine, the above error will occur.</p><h1 id=load-the-model-correctly>Load the model correctly</h1><p>A better way to load a model is to move it to CPU using the <code>map_location</code> parameter of <code>torch.load()</code>.
Load the model to CPU, then load the model parameter into the model, finally, move the model to GPU:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># move the model parameter to cpu</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;my-model.pth&#39;</span><span class=p>,</span> <span class=n>map_location</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cpu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># now move the model parameter to a GPU device of your choice</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cuda:0&#39;</span><span class=p>))</span>
</span></span></code></pre></div><h1 id=ref>ref</h1><ul><li>saving and loading models: <a href=https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html#saving-and-loading-models-across-devices-in-pytorch>https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html#saving-and-loading-models-across-devices-in-pytorch</a></li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-01-28</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/PyTorch/>PyTorch</a></div><nav class=post-nav><a class=prev href=/2022/02/01/stir-fried-shrimp-with-garlic/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">菜谱：蒜蓉炒虾</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/2022/01/14/linkedin_git_assessment/><span class="next-text nav-default">Selected Questions from LinkedIn Git Assessment</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span><span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>