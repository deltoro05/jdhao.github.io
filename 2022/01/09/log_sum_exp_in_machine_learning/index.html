<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Why do We Use LogSumExp in Machine Learning? - jdhao's digital space</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content=" LogSumExp is often used in machine learning. It has the following form:
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.123.8 with theme even"><link rel=canonical href=https://jdhao.github.io/2022/01/09/log_sum_exp_in_machine_learning/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Why do We Use LogSumExp in Machine Learning?"><meta property="og:description" content="


LogSumExp is often used in machine learning. It has the following
form:"><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2022/01/09/log_sum_exp_in_machine_learning/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-01-09T20:39:52+08:00"><meta property="article:modified_time" content="2022-02-27T05:58:45+01:00"><meta itemprop=name content="Why do We Use LogSumExp in Machine Learning?"><meta itemprop=description content="


LogSumExp is often used in machine learning. It has the following
form:"><meta itemprop=datePublished content="2022-01-09T20:39:52+08:00"><meta itemprop=dateModified content="2022-02-27T05:58:45+01:00"><meta itemprop=wordCount content="330"><meta itemprop=keywords content="softmax,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Why do We Use LogSumExp in Machine Learning?"><meta name=twitter:description content="


LogSumExp is often used in machine learning. It has the following
form:"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Why do We Use LogSumExp in Machine Learning?</h1><div class=post-meta><span class=post-time>2022-01-09</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>330 words </span><span class=more-meta>2 mins read </span><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content></div></div><div class=post-content><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/202201092324184.jpg width=600></p><p>LogSumExp is often used in machine learning. It has the following
form:</p><p><span class="math display">\[\begin{equation}
LSE(x_1, x_2, \ldots, x_n) = \log\sum_{i=1}^{N}\exp(x_i)
\end{equation}\]</span></p><h1 id=lse-as-an-upper-bound-for-max>LSE as an upper bound for
max()</h1><p>LSE is an upper bound for <span class="math inline">\(max(x_1, x_2,
\ldots, x_n)\)</span> (The equality establishes only when <span class="math inline">\(n=1\)</span>). We have the following
inequality:</p><p><span class="math display">\[\begin{equation}
max(x_1, x_2, \ldots, x_n) &lt;= \log\sum_{i=1}^{N}\exp(x_i)
\end{equation}\]</span></p><p>To verify this, I have drawn a graph comparing LSE vs max for 2D data
in the range of [-1, 1]. The graph is shown in the title image. It clear
that LSE is indeed an uppber bound for max.</p><details><summary><font size=3 color=red>click here to check the code used to generate
the title image</font></summary><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1 aria-hidden=true tabindex=-1></a><span class=im>import</span> matplotlib.pyplot <span class=im>as</span> plt</span>
<span id=cb1-2><a href=#cb1-2 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-3><a href=#cb1-3 aria-hidden=true tabindex=-1></a><span class=im>import</span> numpy <span class=im>as</span> np</span>
<span id=cb1-4><a href=#cb1-4 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-5><a href=#cb1-5 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-6><a href=#cb1-6 aria-hidden=true tabindex=-1></a><span class=kw>def</span> main():</span>
<span id=cb1-7><a href=#cb1-7 aria-hidden=true tabindex=-1></a>    fig <span class=op>=</span> plt.figure()</span>
<span id=cb1-8><a href=#cb1-8 aria-hidden=true tabindex=-1></a>    ax <span class=op>=</span> fig.add_subplot(projection<span class=op>=</span><span class=st>&#39;3d&#39;</span>)</span>
<span id=cb1-9><a href=#cb1-9 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-10><a href=#cb1-10 aria-hidden=true tabindex=-1></a>    N <span class=op>=</span> <span class=dv>200</span></span>
<span id=cb1-11><a href=#cb1-11 aria-hidden=true tabindex=-1></a>    x1 <span class=op>=</span> np.linspace(<span class=op>-</span><span class=dv>1</span>, <span class=dv>1</span>, N)</span>
<span id=cb1-12><a href=#cb1-12 aria-hidden=true tabindex=-1></a>    x2 <span class=op>=</span> np.linspace(<span class=op>-</span><span class=dv>1</span>, <span class=dv>1</span>, N)</span>
<span id=cb1-13><a href=#cb1-13 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-14><a href=#cb1-14 aria-hidden=true tabindex=-1></a>    X1, X2 <span class=op>=</span> np.meshgrid(x1, x2)</span>
<span id=cb1-15><a href=#cb1-15 aria-hidden=true tabindex=-1></a>    Y <span class=op>=</span> np.log(np.exp(X1) <span class=op>+</span> np.exp(X2))</span>
<span id=cb1-16><a href=#cb1-16 aria-hidden=true tabindex=-1></a>    surf <span class=op>=</span> ax.plot_surface(X1, X2, Y, color<span class=op>=</span><span class=st>&#39;red&#39;</span>)</span>
<span id=cb1-17><a href=#cb1-17 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-18><a href=#cb1-18 aria-hidden=true tabindex=-1></a>    Y_up <span class=op>=</span> np.<span class=bu>max</span>(np.stack([X1, X2], axis<span class=op>=</span><span class=dv>0</span>), axis<span class=op>=</span><span class=dv>0</span>)</span>
<span id=cb1-19><a href=#cb1-19 aria-hidden=true tabindex=-1></a>    surf2 <span class=op>=</span> ax.plot_surface(X1, X2, Y_up, color<span class=op>=</span><span class=st>&#39;blue&#39;</span>)</span>
<span id=cb1-20><a href=#cb1-20 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-21><a href=#cb1-21 aria-hidden=true tabindex=-1></a>    <span class=co># y = np.log(np.exp(x1) + np.exp(x2))</span></span>
<span id=cb1-22><a href=#cb1-22 aria-hidden=true tabindex=-1></a>    <span class=co># ax.plot_trisurf(x1, x2, y, color=&#39;red&#39;)</span></span>
<span id=cb1-23><a href=#cb1-23 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-24><a href=#cb1-24 aria-hidden=true tabindex=-1></a>    <span class=co># y_up = np.max(np.stack([x1, x2], axis=1), axis=1)</span></span>
<span id=cb1-25><a href=#cb1-25 aria-hidden=true tabindex=-1></a>    <span class=co># ax.plot_trisurf(x1, x2, y_up, color=&#39;green&#39;)</span></span>
<span id=cb1-26><a href=#cb1-26 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-27><a href=#cb1-27 aria-hidden=true tabindex=-1></a>    ax.set_xlabel(<span class=st>&quot;X1&quot;</span>)</span>
<span id=cb1-28><a href=#cb1-28 aria-hidden=true tabindex=-1></a>    ax.set_ylabel(<span class=st>&quot;X2&quot;</span>)</span>
<span id=cb1-29><a href=#cb1-29 aria-hidden=true tabindex=-1></a>    ax.set_zlabel(<span class=st>&quot;Z&quot;</span>)</span>
<span id=cb1-30><a href=#cb1-30 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-31><a href=#cb1-31 aria-hidden=true tabindex=-1></a>    <span class=co># change the 3D plot angel and dist, https://stackoverflow.com/q/12904912/6064933</span></span>
<span id=cb1-32><a href=#cb1-32 aria-hidden=true tabindex=-1></a>    ax.view_init(elev<span class=op>=</span><span class=dv>11</span>, azim<span class=op>=-</span><span class=dv>46</span>)</span>
<span id=cb1-33><a href=#cb1-33 aria-hidden=true tabindex=-1></a>    ax.dist <span class=op>=</span> <span class=dv>10</span></span>
<span id=cb1-34><a href=#cb1-34 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-35><a href=#cb1-35 aria-hidden=true tabindex=-1></a>    <span class=co># plt.show()</span></span>
<span id=cb1-36><a href=#cb1-36 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-37><a href=#cb1-37 aria-hidden=true tabindex=-1></a>    plt.savefig(<span class=st>&quot;log_sum_exp_vs_max.pdf&quot;</span>, bbox_inches<span class=op>=</span><span class=st>&#39;tight&#39;</span>)</span>
<span id=cb1-38><a href=#cb1-38 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-39><a href=#cb1-39 aria-hidden=true tabindex=-1></a></span>
<span id=cb1-40><a href=#cb1-40 aria-hidden=true tabindex=-1></a><span class=cf>if</span> <span class=va>__name__</span> <span class=op>==</span> <span class=st>&quot;__main__&quot;</span>:</span>
<span id=cb1-41><a href=#cb1-41 aria-hidden=true tabindex=-1></a>    main()</span></code></pre></div></details><p>How is this information useful? Well, we can use it to transform the
optimization target. For example, you may want to optimize <span class="math inline">\(max(x_1, x_2)\)</span>, which is not
differentiable. Then we can optimize <span class="math inline">\(LSE(x_1, x_2)\)</span> instead.</p><p>In <a href=https://arxiv.org/abs/1511.06452>Lifted structure
loss</a>, they used this trick to transform equation 3 to equation 4.
Without this knowledge, you will find it difficult to understand how
they arrive at equation 4.</p><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/202201100000369.png width=800></p><h1 id=lse-for-numerical-stability>LSE for numerical stability</h1><p>When we use softmax function to normalize a vector and the vector
contains large or small values, we will encounter numerical issues
(overflow or underflow). We need to use LSE to alleviate this issue.</p><p><a href=https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/>This
post</a> explains how it works in detail.</p><h1 id=references>References</h1><ul><li>https://en.wikipedia.org/wiki/LogSumExp</li><li><a href="https://matplotlib.org/stable/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.html?highlight=plot_trisurf#mpl_toolkits.mplot3d.axes3d.Axes3D.plot_trisurf">matplotlib
plot_trisurf</a></li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-02-27</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/softmax/>softmax</a></div><nav class=post-nav><a class=prev href=/2022/01/14/linkedin_git_assessment/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Selected Questions from LinkedIn Git Assessment</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/2022/01/09/git_squash_last_n_commits/><span class="next-text nav-default">How to Squash Last N Commits in Git?</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a><a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a><a href=https://github.com/jdhao class="iconfont icon-github" title=github></a><a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span><span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>