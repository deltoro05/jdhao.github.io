<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>The Warmup Trick for Training Deep Neural Networks - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="Warmup is a training technique often used in training deep neural networks. In this post, I will try to explain what is warmup, and how does it work.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2020/08/14/warmup_maskrcnn_how_does_it_work/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="The Warmup Trick for Training Deep Neural Networks"><meta property="og:description" content="Warmup is a training technique often used in training deep neural networks.
In this post, I will try to explain what is warmup, and how does it work."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2020/08/14/warmup_maskrcnn_how_does_it_work/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-08-14T23:04:54+08:00"><meta property="article:modified_time" content="2022-03-21T17:06:12+01:00"><meta itemprop=name content="The Warmup Trick for Training Deep Neural Networks"><meta itemprop=description content="Warmup is a training technique often used in training deep neural networks.
In this post, I will try to explain what is warmup, and how does it work."><meta itemprop=datePublished content="2020-08-14T23:04:54+08:00"><meta itemprop=dateModified content="2022-03-21T17:06:12+01:00"><meta itemprop=wordCount content="599"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="The Warmup Trick for Training Deep Neural Networks"><meta name=twitter:description content="Warmup is a training technique often used in training deep neural networks.
In this post, I will try to explain what is warmup, and how does it work."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>The Warmup Trick for Training Deep Neural Networks</h1><div class=post-meta><span class=post-time>2020-08-14</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>599 words</span>
<span class=more-meta>3 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#why-do-we-need-warmup>Why do we need warmup</a></li><li><a href=#warmup-applications>Warmup applications</a><ul><li><a href=#warmup-in-resnet>Warmup in ResNet</a></li><li><a href=#how-does-linear-warmup-work-in-maskrcnn>How does linear warmup work in maskrcnn</a></li></ul></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></div><div class=post-content><p>Warmup is a training technique often used in training deep neural networks.
In this post, I will try to explain what is warmup, and how does it work.</p><p>Warmup was originally proposed in this paper: <a href=https://arxiv.org/abs/1706.02677>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a>.
It gives a good explanation on why warmup is needed, and explains different strategies of warmup.</p><h2 id=why-do-we-need-warmup>Why do we need warmup</h2><p>Suppose that we use learning rate $\eta$ on a single GPU with batch size $n$,
when we train the network on 8 GPUs, now the batch size becomes $8n$.
The learning rate also needs to change to suit the distributed training scenario.
The author find that in practice, the linear scaling of learning rate works pretty well.
For example, when we use initial learning rate 0.01 for one GPU,
we may use an initial learning rate of 0.08 for distributed training, i.e., 0.01*8.</p><p>However, to use linear scaling of learning rate, certain condition have to be met<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.
On the initial training stage, due to the rapid change of network parameters,
the condition that makes linear scaling work does not hold any more.
So, in the initial training stage, the authors propose <code>warmup</code> to tackle this issue.</p><p>The basic idea is that we should use a small learning rate than the value calculated by linear scaling policy.
There are two strategies for warmup:</p><ul><li><strong>constant</strong>: Use a low learning rate than 0.08 for the initial few epochs.</li><li><strong>gradual</strong>: In the first few epochs, the learning rate is set to be lower than 0.08
and increased gradually to approach 0.08 as epoch number increases.
In maskrcnn, a <code>linear</code> warmup strategy is used for control warmup factor in
the initial learning stage.</li></ul><p>After the warmup epochs, the learning rate strategy would return to normal.
You can change the learning rate based on the task at hand.</p><h2 id=warmup-applications>Warmup applications</h2><h3 id=warmup-in-resnet>Warmup in ResNet</h3><p>In <a href=https://arxiv.org/abs/1512.03385>Deep residual learning</a>, when training a 110-layer ResNet on CIFAR-10 (section 4.2),
the authors used constant warmup to ease the initial training iterations:</p><blockquote><p>In this case, we find that the initial learning rate of 0.1 is slightly too
large to start converging. So we use 0.01 to warm up the training until the
training error is below 80% (about 400 iterations), and then go back to 0.1
and continue training.</p></blockquote><h3 id=how-does-linear-warmup-work-in-maskrcnn>How does linear warmup work in maskrcnn</h3><p>In <a href=https://github.com/facebookresearch/maskrcnn-benchmark>maskrcnn-benchmark</a>, there is some config parameters about warmup in solver (<code>WARMUP_FACTOR</code>, <code>WARMUP_ITERS</code>, <code>WARMUP_METHOD</code>).
The warmup method used by maskrcnn-benchmark can be found <a href=https://github.com/facebookresearch/maskrcnn-benchmark/blob/57eec25b75144d9fb1a6857f32553e1574177daf/maskrcnn_benchmark/solver/lr_scheduler.py#L39-L52>here</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_lr</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_factor</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>last_epoch</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_iters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_method</span> <span class=o>==</span> <span class=s2>&#34;constant&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>warmup_factor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_factor</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_method</span> <span class=o>==</span> <span class=s2>&#34;linear&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>alpha</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>last_epoch</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_iters</span>
</span></span><span class=line><span class=cl>            <span class=n>warmup_factor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_factor</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha</span><span class=p>)</span> <span class=o>+</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>base_lr</span>
</span></span><span class=line><span class=cl>        <span class=o>*</span> <span class=n>warmup_factor</span>
</span></span><span class=line><span class=cl>        <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>**</span> <span class=n>bisect_right</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>milestones</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>last_epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>base_lr</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>base_lrs</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span></code></pre></div><p>In the above code, <code>self.last_epoch</code> is the current training iteration
(because maskrcnn-benchmark use iteration instead of the usual epoch to measure the training process).
<code>self.warmup_iters</code> is the number of iterations for warmup in the initial training stage.
<code>self.warmup_factors</code> are a constant (0.333 in this case).</p><p>Only when current iteration number is below <code>self.warmup_iters</code>, will the <code>warmup_factor</code> be used.
Otherwise, it will be 1 and not affect the learning rate.</p><p>When current iteration is below <code>warmup_iters</code> and warmup method is <code>linear</code>.
The warmup factor used is calculated as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>warmp_factor</span> <span class=o>=</span> 0.667 * <span class=o>(</span>current_iter/warmup_iters<span class=o>)</span> + 0.333
</span></span></code></pre></div><p>So as current iteration approaches <code>warmup_iters</code>, <code>warmup_factor</code> will gradually approach 1.
As a result, the learning rate used will approach base learning rate.</p><h2 id=references>References</h2><ul><li><a href=https://github.com/facebookresearch/maskrcnn-benchmark/issues/562>How the learning rate change?</a></li><li><a href=https://www.reddit.com/r/MachineLearning/comments/es9qv7/d_warmup_vs_initially_high_learning_rate/>Discussions about warmup policy</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>see section 2.1 of the paper for details.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-03-21</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/2020/08/18/pillow_create_text_outline/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Create Outline Text in Python with Pillow</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2020/08/06/miui12_clipboard_issue/><span class="next-text nav-default">MIUI 12 输入法无法复制文本到剪切板</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>