<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>1x1 Convolutions Demystified - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="In the early development of convolutional neural networks (CNNs), convolutions with kernel size \(3\times 3\), \(5\times 5\), \(7\times 7\) or even \(11\times 11\) are often used. In the more recent literature, however, \(1\times 1\) convolutions are becoming prevalent. In this post, I will try to explain what \(1\times 1\) convolutions are and discuss why they are used in CNNs.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="1x1 Convolutions Demystified"><meta property="og:description" content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size \(3\times
3\), \(5\times 5\), \(7\times 7\) or even \(11\times 11\) are often used. In the more
recent literature, however, \(1\times
1\) convolutions are becoming prevalent. In this post, I will try
to explain what \(1\times 1\)
convolutions are and discuss why they are used in CNNs."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-09-29T09:40:00+08:00"><meta property="article:modified_time" content="2022-03-21T16:33:00+01:00"><meta itemprop=name content="1x1 Convolutions Demystified"><meta itemprop=description content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size \(3\times
3\), \(5\times 5\), \(7\times 7\) or even \(11\times 11\) are often used. In the more
recent literature, however, \(1\times
1\) convolutions are becoming prevalent. In this post, I will try
to explain what \(1\times 1\)
convolutions are and discuss why they are used in CNNs."><meta itemprop=datePublished content="2017-09-29T09:40:00+08:00"><meta itemprop=dateModified content="2022-03-21T16:33:00+01:00"><meta itemprop=wordCount content="755"><meta itemprop=keywords content="CNN,"><meta name=twitter:card content="summary"><meta name=twitter:title content="1x1 Convolutions Demystified"><meta name=twitter:description content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size \(3\times
3\), \(5\times 5\), \(7\times 7\) or even \(11\times 11\) are often used. In the more
recent literature, however, \(1\times
1\) convolutions are becoming prevalent. In this post, I will try
to explain what \(1\times 1\)
convolutions are and discuss why they are used in CNNs."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>1x1 Convolutions Demystified</h1><div class=post-meta><span class=post-time>2017-09-29</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>755 words</span>
<span class=more-meta>4 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content></div></div><div class=post-content><p>In the early development of convolutional neural networks (CNNs),
convolutions with kernel size <span class="math inline">\(3\times
3\)</span>, <span class="math inline">\(5\times 5\)</span>, <span class="math inline">\(7\times 7\)</span> or even <span class="math inline">\(11\times 11\)</span> are often used. In the more
recent literature, however, <span class="math inline">\(1\times
1\)</span> convolutions are becoming prevalent. In this post, I will try
to explain what <span class="math inline">\(1\times 1\)</span>
convolutions are and discuss why they are used in CNNs.</p><h1 id=the-meaning-of-1times-1-convolutions>The meaning of <span class="math inline">\(1\times 1\)</span> convolutions</h1><p>In essence, <span class="math inline">\(1\times 1\)</span>
convolutions are just convolutions with kernel size equal to <span class="math inline">\(1\)</span>, nothing new. Suppose the feature map
size of one layer is <span class="math inline">\(64\times 12\times
12\)</span>, and it is followed by <span class="math inline">\(1\times
1\)</span> convolutions, and the output channel is <span class="math inline">\(C\)</span>. Then in this convolution layer, we
will have <span class="math inline">\(C\)</span> filters, each of which
has shape <span class="math inline">\(64\times 1\times 1\)</span>. The
convolutional weight shape in this layer is <span class="math inline">\(C\times 64\times 1\times 1\)</span>, and the shape
of bias terms is <span class="math inline">\(C\times 1\times 1\)</span>.
The output feature map size after this convolutional layer is <span class="math inline">\(C\times 12\times 12\)</span>. So the spatial size
of feature maps is preserved, but the number of feature maps are changed
from 64 to <span class="math inline">\(C\)</span>.</p><p>The paper <a href=https://arxiv.org/abs/1312.4400><em>Network in
network</em></a> first proposed to use <span class="math inline">\(1\times 1\)</span> convolutions. In that paper,
multiple <span class="math inline">\(1\times 1\)</span> convolutional
layers is stacked together, which the authors call a <em>mlp layer</em>.
Since the kernel size is <span class="math inline">\(1\times 1\)</span>,
which can not learn a spatial relationship within channels, it is used
solely to learn cross-channel relations of the feature maps. You can
also think of it as a fully-connected operation, where for each feature
map the multiplying coefficients are the same.</p><p>Later in the Google <a href=https://arxiv.org/abs/1409.4842>Inception</a> paper, <span class="math inline">\(1\times 1\)</span> convolutions are also used in
the inception module shown below.</p><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/google_inception-V1.png width=600></p><p>The use of <span class="math inline">\(1\times 1\)</span>
convolutions is mainly for dimension reduction in channels to reduce the
computational cost of the following <span class="math inline">\(3\times
3\)</span> or <span class="math inline">\(5\times 5\)</span>
convolutions. Another purpose is to increase the non-linearity as <span class="math inline">\(1\times 1\)</span> convolutions are immediately
followed by ReLU. The two purposes of <span class="math inline">\(1\times 1\)</span> convolutions are stated clearly
in this paper:</p><blockquote><p>That is, <span class="math inline">\(1\times 1\)</span> convolutions
are used to compute reductions before the expensive <span class="math inline">\(3\times 3\)</span> and <span class="math inline">\(5\times 5\)</span> convolutions. Besides being
used as reductions, they also include the use of rectified linear
activation making them dual-purpose.</p></blockquote><p>In the more recent <a href=https://arxiv.org/abs/1512.03385>ResNet</a> paper, the authors
proposed to use “bottleneck” structure (shown below), which also
involves the use of <span class="math inline">\(1\times 1\)</span>
convolutions. The purpose of it is also for reducing or increasing
dimensions.</p><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/resnet_bottleneck.png width=300></p><h1 id=relationship-between-fully-connected-layers-and-1times-1-convolutions>Relationship
between fully-connected layers and <span class="math inline">\(1\times
1\)</span> convolutions</h1><p>In image classification, researchers often use fixed size input image
and fully connected layers as classifiers to classify the image. But
according to the father of convolutional neural networks – Yann lecun,
there is no such thing as fully-connected layers and you do not need to
use fixed size input image in testing time:</p><blockquote><p>In Convolutional Nets, there is no such thing as “fully-connected
layers”. There are only convolution layers with 1x1 convolution kernels
and a full connection table. It’s a too-rarely-understood fact that
ConvNets don’t need to have a fixed-size input. You can train them on
inputs that happen to produce a single output vector (with no spatial
extent), and then apply them to larger images. Instead of a single
output vector, you then get a spatial map of output vectors. Each vector
sees input windows at different locations on the input. In that
scenario, the “fully connected layers” really act as 1x1
convolutions.</p></blockquote><p>That is, we can interpret fully-connected layers as <span class="math inline">\(1\times 1\)</span> convolutions. For example, in
VGG-16 network, the input image size is <span class="math inline">\(224\times 224\)</span>, the output is a
classification score for 1000 classes, whose size is <span class="math inline">\(1000\times 1\times 1\)</span>. If the input images
have bigger size than <span class="math inline">\(224\times
224\)</span>, we can interpret the fully-connected layers as
convolutions in a sliding windows fashion so that the output of the
network is a classification map of size <span class="math inline">\(1000\times W\times H\)</span>, where <span class="math inline">\(W>1\)</span> and <span class="math inline">\(H>1\)</span>. Each point in each classification
map represent the probability of the corresponding receptive field in
the original image belonging to that specific class.</p><h1 id=conclusions>Conclusions</h1><p>In summary, <span class="math inline">\(1\times 1\)</span>
convolutions are used for 2 purposes: ones is to reduce dimensions in
order to reduce computational cost, the other is to increase
non-linearity of the network (or the capacity of the network) in a cheap
way.</p><h1 id=references>References</h1><ul><li><a href=https://groups.google.com/forum/#!topic/caffe-users/f1R-JrUQSMg>A
discussion about the meaning of <span class="math inline">\(1\times
1\)</span> convolutions</a></li><li><a href=https://stackoverflow.com/q/39366271/6064933>Why <span class="math inline">\(1\times 1\)</span> convolutions are used</a></li><li><a href=https://stats.stackexchange.com/q/194142/140049>Another
post about the meaning of <span class="math inline">\(1\times 1\)</span>
convolutions</a></li><li><a href=https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/>A
discussion about the significance of <em>network in
network</em></a></li><li><a href=https://qr.ae/pG08uW>How to interpret the fully-connected
layers as convolutions</a>.</li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-03-21</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/CNN/>CNN</a></div><nav class=post-nav><a class=prev href=/2017/09/30/sorting-algorithms-stability/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">When Does the Stability of Sorting Algorithms Matter?</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/09/24/some-good-android-apps/><span class="next-text nav-default">善用佳软---我常用的 Android 应用</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>