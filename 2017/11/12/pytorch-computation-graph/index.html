<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Understanding Computational Graphs in PyTorch - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="PyTorch is a relatively new deep learning library which support dynamic computation graphs. It has gained a lot of attention after its official release in January. In this post, I want to share what I have learned about the computation graph in PyTorch. Without basic knowledge of computation graph, we can hardly understand what is actually happening under the hood when we are trying to train our landscape-changing neural networks.
"><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2017/11/12/pytorch-computation-graph/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Understanding Computational Graphs in PyTorch"><meta property="og:description" content="PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our landscape-changing neural networks."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2017/11/12/pytorch-computation-graph/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-11-12T13:22:46+08:00"><meta property="article:modified_time" content="2022-01-09T14:10:45+01:00"><meta itemprop=name content="Understanding Computational Graphs in PyTorch"><meta itemprop=description content="PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our landscape-changing neural networks."><meta itemprop=datePublished content="2017-11-12T13:22:46+08:00"><meta itemprop=dateModified content="2022-01-09T14:10:45+01:00"><meta itemprop=wordCount content="688"><meta itemprop=keywords content="PyTorch,optimization,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Computational Graphs in PyTorch"><meta name=twitter:description content="PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our landscape-changing neural networks."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Understanding Computational Graphs in PyTorch</h1><div class=post-meta><span class=post-time>2017-11-12</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>688 words</span>
<span class=more-meta>4 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#computation-graphs-and-its-use-in-pytorch>Computation graphs and its use in PyTorch</a></li><li><a href=#how-is-computation-graph-created-and-freed>How is computation graph created and freed?</a></li><li><a href=#a-toy-example>A toy example</a></li><li><a href=#real-use-cases>Real use cases</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=post-content><p>PyTorch is a relatively new deep learning library which support dynamic
computation graphs. It has gained a lot of attention after its official release
in January. In this post, I want to share what I have learned about the
computation graph in PyTorch. Without basic knowledge of computation graph, we
can hardly understand what is actually happening under the hood when we are
trying to train our <em>landscape-changing</em> neural networks.</p><h1 id=computation-graphs-and-its-use-in-pytorch>Computation graphs and its use in PyTorch</h1><p>The idea of <a href=http://colah.github.io/posts/2015-08-Backprop/>computation graph</a>
is important in the optimization of large-scale neural networks. In simple
terms, a computation graph is a <a href=https://stackoverflow.com/q/2283757/6064933>DAG</a>
in which nodes represent variables (tensors, matrix, scalars, etc.) and edge
represent some mathematical operations (for example, summation,
multiplication). The computation graph has some leaf variables. The root
variables of the graph are computed according to operations defined by the
graph. During the optimization step, we combine the chain rule and the graph to
compute the derivative of the output w.r.t the learnable variable in the graph
and update these variables to make the output close to what we want. In neural
networks, these learnable variables are often called weight and bias.</p><p>You can also think of neural network as a computational graph: the input images
and the <a href=http://pytorch.org/docs/master/nn.html#torch.nn.Parameter>parameters</a>
in each layer are leaf variables, the outputs (usually it is called the loss
and we minimize it to update the parameters of the network) of neural networks
are the root variables in the graph.</p><h1 id=how-is-computation-graph-created-and-freed>How is computation graph created and freed?</h1><p>In PyTorch, the computation graph is created for each iteration in an epoch. In
each iteration, we execute the forward pass, compute the derivatives of output
w.r.t to the parameters of the network, and update the parameters to fit the
given examples. After doing the backward pass, the graph will be freed to save
memory. In the next iteration, a fresh new graph is created and ready for
back-propagation.</p><p>Because the computation graph will be freed by default after the first backward
pass, you will encounter errors if you are trying to do backward on the same
graph the second time. That is why the following error message pops up:</p><blockquote><p>RuntimeError: Trying to backward through the graph a second time, but the
buffers have already been freed. Specify retain_graph=True when calling
backward the first time</p></blockquote><h1 id=a-toy-example>A toy example</h1><p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/20200214143225.png></p><p>Now, let&rsquo;s take a small example to illustrate the idea. Suppose that we have a
computation graph shown above. The variable <code>d</code> and <code>e</code> is the output, and <code>a</code>
is the input. The underlining computation is:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>a</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>b</span><span class=o>*</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>d</span> <span class=o>=</span> <span class=n>c</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>e</span> <span class=o>=</span> <span class=n>c</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span></code></pre></div><p>when we do <code>d.backward()</code>, that is fine. After this computation, the part of
graph that calculate <code>d</code> will be freed by default to save memory. So if we do
<code>e.backward()</code>, the error message will pop up. In order to do <code>e.backward()</code>,
we have to set the parameter <code>retain_graph</code> to <code>True</code> in <code>d.backward()</code>, i.e.,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>d</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>As long as you use <code>retain_graph=True</code> in your backward method, you can do backward any time you want:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>d</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># fine</span>
</span></span><span class=line><span class=cl><span class=n>e</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># fine</span>
</span></span><span class=line><span class=cl><span class=n>d</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span> <span class=c1># also fine</span>
</span></span><span class=line><span class=cl><span class=n>e</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span> <span class=c1># error will occur!</span>
</span></span></code></pre></div><h1 id=real-use-cases>Real use cases</h1><p>A real use case that you want to backward through the graph for more than once
is multi-task learning where you have multiple losses at different layers.
Suppose that you have 2 losses: <code>loss1</code> and <code>loss2</code> and they reside in
different layers. In order to back-prop the gradient of <code>loss1</code> and <code>loss2</code>
w.r.t to the learnable weight of your network independently. You have to use
<code>retain_graph=True</code> in <code>backward()</code> method in the first back-propagated loss.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># suppose you first back-propagate loss1, then loss2</span>
</span></span><span class=line><span class=cl><span class=c1># (you can also do it in reverse order)</span>
</span></span><span class=line><span class=cl><span class=n>loss1</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss2</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span> <span class=c1># now the graph is freed, and next process of batch gradient descent is ready</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span> <span class=c1># update the network parameters</span>
</span></span></code></pre></div><h1 id=references>References</h1><ul><li><a href=https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308>leaf variable in PyTorch</a>.</li><li><a href=https://discuss.pytorch.org/t/which-is-freed-which-is-not/8636/2>How is computation graph created</a></li><li><a href=https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795>Error trying to backward the second time</a>.</li><li><a href=https://discuss.pytorch.org/t/understanding-graphs-and-state/224>Understanding graph and state</a>.</li><li><a href=https://discuss.pytorch.org/t/how-computation-graph-in-pytorch-is-created-and-freed/3515>Graph creation and destroy</a>.</li><li><a href=https://stackoverflow.com/q/46774641/6064933>What does <code>retain_graph</code> in <code>backward()</code> function mean</a>?</li><li><a href=http://colah.github.io/posts/2015-08-Backprop/>Back-propagation on computation graph</a>.</li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-01-09</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/PyTorch/>PyTorch</a>
<a href=/tags/optimization/>optimization</a></div><nav class=post-nav><a class=prev href=/2017/11/15/pytorch-datatype-note/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Notes on PyTorch Tensor Data Types</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/11/06/resize-image-to-square-with-padding/><span class="next-text nav-default">How to Resize, Pad Image to Square Shape and Keep Its Aspect Ratio in Python</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>